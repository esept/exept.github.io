<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Diary-05-2020</title>
    <url>/2020/05/10/diary/</url>
    <content><![CDATA[<p>这里曾经，依旧是我的五月<br>岁月漫长，但是值得等待<br> <a id="more"></a></p>
<h2 id="2020-05"><a href="#2020-05" class="headerlink" title="2020-05"></a>2020-05</h2><p>自己剖析自己是一件挺可怕的事情，因为有的时候一旦发现自己做那件事的原因竟然是因为这样的一个理由，如果这样的原因是一个不愿被自己接受的结果，那面对自己都会有一点不情愿<br>对于爬虫这个东西真的是麻烦，做个项目竟然还要自己购买代理</p>
<p>今天不知道是第几次试图创建图片流失败了<br>我基本上已经要放弃尝试了，现在前端的东西一点也不会，完全就是按照网上的教程来进行自己的操作<br>然后嘞，我发现现在这样子的一个环境还是很适合自己的<br>因为很有感觉<br>然后发现自己创建更新内容和前端，访问量轻轻松松上百</p>
<p>Emmm。。。</p>
<p>想回国的时候去北京吃一堆想吃的东西</p>
<p>今天先这样子吧</p>
<p>喜欢下雨天。</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>采集忘却</tag>
      </tags>
  </entry>
  <entry>
    <title>他是诗人</title>
    <url>/2020/05/08/doc/</url>
    <content><![CDATA[<p>他曾说 我爱你<br>就像叶爱飘零 风爱游荡<br>他曾禁锢在个人世界<br>无人读懂 欢喜惆怅<br> <a id="more"></a></p>
<p>让岁月抚平忧伤<br>年华换成怅惘<br>星空月夜伴随凄凉<br>独自扼杀希望</p>
<p>谁说诗人习惯独自流浪<br>只不过从未找到过家的方向<br>他在尝试抚平心灵创伤<br>用诗来抒发梦想</p>
<p>他也曾尝过爱情<br>决绝 苦涩 需要勇气<br>他不许任何人亵渎这经历<br>就像他人亵渎自己</p>
<p>他早迷失在混乱世界<br>等待爱人将他救起<br>他却说我是诗人<br>悲伤才是对的记忆</p>
<p>他在等人帮他抹去泪水<br>等人填补空洞心伤<br>诗人强撑着精神病体<br>说着自由 理应继续彷徨</p>
<p>现在的我向往自由与孤独<br>现在的我甘愿流浪<br>诗人不停地大声吟唱<br>天空的雨滴浸透出点点荒凉</p>
<p>诗人的梦里有他想要的一切<br>却不知从何记起<br>他在夜里悲伤的哭泣<br>谁又见过他的无助模样</p>
<p>他还在用低沉的声音吟唱<br>唱出对世界无尽的失望<br>海阔天空和万丈悬崖其实一样<br>都会抛下久违的梦想</p>
<p>诗人披着月光<br>低吟清唱着他的忧伤<br>他说坚强一点<br>就像月光不惧朝阳</p>
<p>他在慢慢游荡<br>渐渐走向陌生的远方<br>他也不想背井离乡<br>只因这里过分凄凉</p>
<p>其实诗人早就死亡<br>只剩他的躯体被迫流浪<br>他的灵魂灰飞烟灭<br>却无人惋惜他的坚强</p>
<p>你是否曾望见过诗人的孤寂背影<br>就像月光才是最柔美的光芒<br>人们不知是否该去祭奠<br>来安抚孤魂的游荡带来的离殇</p>
<p>固执的他 说 我是诗人<br>怎敢以流泪缓解悲伤<br>他说我喜欢孤身一人 只身迎接<br>坎坷路途上的天堂</p>
]]></content>
      <categories>
        <category>思考与遐想</category>
      </categories>
      <tags>
        <tag>采集忘却</tag>
      </tags>
  </entry>
  <entry>
    <title>大多数的我</title>
    <url>/2020/05/06/%E5%A4%A7%E5%A4%9A%E6%95%B0%E7%9A%84%E6%88%91/</url>
    <content><![CDATA[<p>少数的我<br>梦见了故乡</p>
<a id="more"></a>
<p>从未回去那里的魂魄<br>抢夺去了风中<br>应见泪水相拥的画面<br>战乱连年<br>仅有的雨也成了雪<br>融入这暴戾的暖炉<br>痛不知思绪的去向<br>只能肆意地游荡在躯体<br>千万之一相见的几率<br>释怀于死前<br>无言的喧寂<br>大多数的我<br>梦不见故乡<br>身处于不知何方 无可奈何<br>指引的信号刻画于手臂<br>感官隐于黑暗<br>轻道  恭喜</p>
]]></content>
      <categories>
        <category>思考与遐想</category>
      </categories>
      <tags>
        <tag>采集忘却</tag>
      </tags>
  </entry>
  <entry>
    <title>如果我想你</title>
    <url>/2020/05/08/%E5%A6%82%E6%9E%9C%E6%88%91%E6%83%B3%E4%BD%A0/</url>
    <content><![CDATA[<p>愿你能从他身上听到你想听的话<br>单调却挂念的话<br>有没有一些<br>来自我这个人渣</p>
<a id="more"></a>
<p>记忆的沙 霜冻的画<br>故乡的花已经发芽<br>牵着希望的手悄然滑下<br>我还是怀念我残破的家<br>音乐盒的主人公默念牵挂<br>没有你的口信<br>我不敢安家<br>你还留着我寄存的味道吗<br>书写下的话 让天空散漫着花<br>那景象是我从未见过的<br>我想问你可知爱字如何落下<br>却无法否认那是我从未见过的回答</p>
]]></content>
      <categories>
        <category>思考与遐想</category>
      </categories>
      <tags>
        <tag>采集忘却</tag>
      </tags>
  </entry>
  <entry>
    <title>Scrapy爬虫</title>
    <url>/2020/05/13/%E7%88%AC%E8%99%AB/</url>
    <content><![CDATA[<p>python-scrapy学习中所做的爬取某一招标网站的内容</p>
<a id="more"></a>

<h1 id="模拟请求头与代理IP"><a href="#模拟请求头与代理IP" class="headerlink" title="模拟请求头与代理IP"></a>模拟请求头与代理IP</h1><h2 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h2><ol>
<li>找到目标文件夹</li>
<li>创建项目  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ scrapy startproject &#39;ProjectName&#39;</span><br><span class="line">$ cd &#39;ProjectName&#39;</span><br><span class="line">$ scrapy genspider &#39;Name&#39; &quot;网站域名&quot;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="配置Setting"><a href="#配置Setting" class="headerlink" title="配置Setting"></a>配置Setting</h2><ol>
<li><p>关闭Robot协议</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br></pre></td></tr></table></figure></li>
<li><p>开启Download_Delay</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DOWNLOAD_DELAY = <span class="number">3</span></span><br></pre></td></tr></table></figure></li>
<li><p>设置默认请求头</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">  <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span>,</span><br><span class="line">  <span class="string">'Accept-Language'</span>: <span class="string">'en'</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>开启下载中间件与管道文件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">'zhaobiao.middlewares.ZhaobiaoDownloaderMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'zhaobiao.pipelines.ZhaobiaoPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>设置请求头 Cookie和User-Agent</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Cookie'</span>:<span class="string">'登陆后的cookie'</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Spiders-项目文件"><a href="#Spiders-项目文件" class="headerlink" title="Spiders/项目文件"></a>Spiders/项目文件</h2><h3 id="设置存储的数据格式"><a href="#设置存储的数据格式" class="headerlink" title="设置存储的数据格式"></a>设置存储的数据格式</h3><p>即爬虫项目里需要爬取的内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sql_data = dict(</span><br><span class="line">      projectcode=<span class="string">''</span>,  <span class="comment"># 项目编号</span></span><br><span class="line">      web=<span class="string">''</span>,  <span class="comment"># 信息来源网站</span></span><br><span class="line">      keyword=<span class="string">''</span>,  <span class="comment"># 关键字</span></span><br><span class="line">      detail_url=<span class="string">''</span>,  <span class="comment"># 招标详细页网址</span></span><br><span class="line">      title=<span class="string">''</span>,  <span class="comment"># 第三方网站发布标题</span></span><br><span class="line">      toptype=<span class="string">''</span>,  <span class="comment"># 信息类型</span></span><br><span class="line">      province=<span class="string">''</span>,  <span class="comment"># 归属省份</span></span><br><span class="line">      product=<span class="string">''</span>,  <span class="comment"># 产品范畴</span></span><br><span class="line">      industry=<span class="string">''</span>,  <span class="comment"># 归属行业</span></span><br><span class="line">      tendering_manner=<span class="string">''</span>,  <span class="comment"># 招标方式</span></span><br><span class="line">      publicity_date=<span class="string">''</span>,  <span class="comment"># 招标公示日期</span></span><br><span class="line">      expiry_date=<span class="string">''</span>,  <span class="comment"># 招标截止时间</span></span><br><span class="line">  )</span><br></pre></td></tr></table></figure>

<h3 id="设置Form表单的数据格式"><a href="#设置Form表单的数据格式" class="headerlink" title="设置Form表单的数据格式"></a>设置Form表单的数据格式</h3><p>即从网站可以获取的数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">form_data = dict(</span><br><span class="line">       infoClassCodes=<span class="string">''</span>,</span><br><span class="line">       rangeType=<span class="string">''</span>,</span><br><span class="line">       projectType=<span class="string">'bid'</span>,</span><br><span class="line">       fundSourceCodes=<span class="string">''</span>,</span><br><span class="line">       dateType=<span class="string">''</span>,</span><br><span class="line">       startDateCode=<span class="string">''</span>,</span><br><span class="line">       endDateCode=<span class="string">''</span>,</span><br><span class="line">       normIndustry=<span class="string">''</span>,</span><br><span class="line">       normIndustryName=<span class="string">''</span>,</span><br><span class="line">       zone=<span class="string">''</span>,</span><br><span class="line">       zoneName=<span class="string">''</span>,</span><br><span class="line">       zoneText=<span class="string">''</span>,</span><br><span class="line">       key=<span class="string">''</span>,  <span class="comment"># 搜索的关键字</span></span><br><span class="line">       pubDateType=<span class="string">''</span>,</span><br><span class="line">       pubDateBegin=<span class="string">''</span>,</span><br><span class="line">       pubDateEnd=<span class="string">''</span>,</span><br><span class="line">       sortMethod=<span class="string">'timeDesc'</span>,</span><br><span class="line">       orgName=<span class="string">''</span>,</span><br><span class="line">       currentPage=<span class="string">''</span>,  <span class="comment"># 当前页码</span></span><br><span class="line">   )</span><br></pre></td></tr></table></figure>
<h3 id="定义start-requests-self"><a href="#定义start-requests-self" class="headerlink" title="定义start_requests(self)"></a>定义start_requests(self)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">    form_data = self.form_data</span><br><span class="line">    form_data[<span class="string">'key'</span>] = <span class="string">'路由器'</span></span><br><span class="line">    form_data[<span class="string">'currentPage'</span>] = <span class="string">'2'</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">yield</span> scrapy.FormRequest( <span class="comment"># 对FormRequest的封装，可以提交表单数据</span></span><br><span class="line">        url = <span class="string">'http://ss.ebnew.com/tradingSearch/index.htm'</span>,</span><br><span class="line">        formdata= form_data,<span class="comment"># 传递字典对象</span></span><br><span class="line">        callback=self.parse_page1,<span class="comment"># 定义回调函数</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page1</span><span class="params">(self,response)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'2.html'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(response.body) <span class="comment"># 写入到本地</span></span><br></pre></td></tr></table></figure>

<h3 id="创建start文件"><a href="#创建start文件" class="headerlink" title="创建start文件"></a>创建start文件</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line"></span><br><span class="line">cmdline.execute(<span class="string">'scrapy crawl bilian'</span>.split())</span><br></pre></td></tr></table></figure>

<h2 id="配置代理ip"><a href="#配置代理ip" class="headerlink" title="配置代理ip"></a>配置代理ip</h2><p>找到middlewares里的class ZhaobiaoDownloaderMiddleware 里的<br>process_request</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 需要倒入包</span></span><br><span class="line"><span class="keyword">import</span> urllib.request <span class="keyword">as</span> ur <span class="comment"># 使用ur.urlopen</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = <span class="string">'http://'</span> + ur.urlopen(<span class="string">'代理IP地址'</span>).read().decode(<span class="string">'utf-8'</span>).strip() <span class="comment"># 设置代理</span></span><br></pre></td></tr></table></figure>
<p>.strip（）去掉首位换行符</p>
<h1 id="页面中的数据过滤提取"><a href="#页面中的数据过滤提取" class="headerlink" title="页面中的数据过滤提取"></a>页面中的数据过滤提取</h1><h2 id="定义parse-page1"><a href="#定义parse-page1" class="headerlink" title="定义parse_page1"></a>定义parse_page1</h2><p>从网页中寻找有需要的信息，并用xpath标记出来</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page1</span><span class="params">(self,response)</span>:</span></span><br><span class="line">    content_list_x_s = response.xpath(<span class="string">'//div[@class="ebnew-content-list"]/div'</span>)</span><br><span class="line">    <span class="keyword">for</span> content_list_x <span class="keyword">in</span> content_list_x_s:</span><br><span class="line">        toptype = content_list_x.xpath(<span class="string">'./div[1]/i[1]/text()'</span>).extract_first()</span><br><span class="line">        title = content_list_x.xpath(<span class="string">'./div[1]/a[1]/text()'</span>).extract_first()</span><br><span class="line">        publicity_date =content_list_x.xpath(<span class="string">'./div[1]/i[2]/text()'</span>).extract_first()</span><br><span class="line">        <span class="keyword">if</span> publicity_date:</span><br><span class="line">            publicity_date = re.sub(<span class="string">'[^0-9\-]'</span>,<span class="string">''</span>,publicity_date) <span class="comment"># 将发布日期里的文字'发布日期'替换成空字符串的格式</span></span><br><span class="line"></span><br><span class="line">        tendering_manner = content_list_x.xpath(<span class="string">'./div[2]/div[1]/p[1]/span[2]/text()'</span>).extract_first()</span><br><span class="line">        product = content_list_x.xpath(<span class="string">'./div[2]/div[1]/p[2]/span[2]/text()'</span>).extract_first()</span><br><span class="line">        expiry_date = content_list_x.xpath(<span class="string">'./div[2]/div[2]/p[1]/span[2]/text()'</span>).extract_first()</span><br><span class="line">        province = content_list_x.xpath(<span class="string">'./div[2]/div[2]/p[2]/span[2]/text()'</span>).extract_first()</span><br><span class="line">        <span class="comment"># print(toptype,title,publicity_date,tendering_manner,product,expiry_date,province)</span></span><br></pre></td></tr></table></figure>

<h2 id="定义parse-page2"><a href="#定义parse-page2" class="headerlink" title="定义parse_page2"></a>定义parse_page2</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">        url = <span class="string">'http://www.ebnew.com/businessShow/645672039.html'</span>,</span><br><span class="line">        callback = self.parse_page(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page2</span><span class="params">(self,response)</span>:</span></span><br><span class="line">    li_x_s = response.xpath(<span class="string">'//ul[@class="ebnew-project-information pd-t25 pd-b25 pd-l20 clearfix ebnew-border bg-f9 mg-t20 color-444 font-12"]/li'</span>)</span><br><span class="line">    projectcode = li_x_s[<span class="number">0</span>].xpath(<span class="string">'./span[2]/text()'</span>).extract_first()</span><br><span class="line">    industry = li_x_s[<span class="number">7</span>].xpath(<span class="string">'./span[2]/text()'</span>).extract_first()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> projectcode: <span class="comment"># 在没有找到项目编号的情况下使用正则表达式来寻找</span></span><br><span class="line">        projectcode_find = re.findall(<span class="string">'项目编号[：:]&#123;0,1&#125;\s&#123;0,2&#125;([a-zA-Z0-9\-]&#123;10,80&#125;)'</span>,response.body.decode(<span class="string">'utf-8'</span>))</span><br><span class="line">        projectcode = projectcode_find[<span class="number">0</span>] <span class="keyword">if</span> projectcode_find <span class="keyword">else</span> <span class="string">""</span></span><br><span class="line">    <span class="comment"># print(industry,projectcode)</span></span><br></pre></td></tr></table></figure>


<h1 id="异常请求的处理机制"><a href="#异常请求的处理机制" class="headerlink" title="异常请求的处理机制"></a>异常请求的处理机制</h1><p>在setting中加入</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">LOG_FILE= <span class="string">'zhaobiao.log'</span> <span class="comment"># 将错误写入到这个错误文件中</span></span><br><span class="line">LOG_LEVEL = <span class="string">'ERROR'</span> <span class="comment"># 定义错误写入的错误级别</span></span><br></pre></td></tr></table></figure>

<h1 id="请求链的健壮性优化"><a href="#请求链的健壮性优化" class="headerlink" title="请求链的健壮性优化"></a>请求链的健壮性优化</h1><h2 id="首先定义关键字"><a href="#首先定义关键字" class="headerlink" title="首先定义关键字"></a>首先定义关键字</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keyword_s =[</span><br><span class="line">    <span class="string">'路由器'</span>,<span class="string">'变压器'</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h2 id="注释异常请求的处理机制"><a href="#注释异常请求的处理机制" class="headerlink" title="注释异常请求的处理机制"></a>注释异常请求的处理机制</h2><h2 id="一些细小的修改"><a href="#一些细小的修改" class="headerlink" title="一些细小的修改"></a>一些细小的修改</h2><ol>
<li>在allowed_domaine中加入参数，因为start_request中有’<a href="http://ss.ebnew.com/tradingSearch/index.htm&#39;" target="_blank" rel="noopener">http://ss.ebnew.com/tradingSearch/index.htm&#39;</a><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">allowed_domains = [<span class="string">'ebnew'</span>,<span class="string">'ss.ebnew.com'</span>]</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h1 id="异步IO实现数据持久化储存"><a href="#异步IO实现数据持久化储存" class="headerlink" title="异步IO实现数据持久化储存"></a>异步IO实现数据持久化储存</h1><h2 id="PyMysql"><a href="#PyMysql" class="headerlink" title="PyMysql"></a>PyMysql</h2><p>连接本地数据库‘pipelines.py’</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql <span class="comment"># 倒入包</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.mysql_conn = pymysql.connections(</span><br><span class="line">            host = <span class="string">'localhost'</span>,</span><br><span class="line">            post=<span class="number">3306</span>,</span><br><span class="line">            user=<span class="string">'root'</span>,</span><br><span class="line">            password = <span class="string">'126511'</span>,</span><br><span class="line">            database=<span class="string">'zhaobiao'</span>,</span><br><span class="line">            charset=<span class="string">'utf8'</span>,</span><br><span class="line"></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> zhaobiao <span class="keyword">charset</span> utf8 <span class="comment"># 创建数据库</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_zhaobiao( <span class="comment"># 创建表格</span></span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span> primary <span class="keyword">key</span> auto_increment,</span><br><span class="line">    projectcode <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    web <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    keyword <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    detail_url <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    title <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    toptype <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    province <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    product <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    industry <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    tendering_manner <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    publicity_date <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">    expiry_date <span class="built_in">varchar</span>(<span class="number">100</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>创建光标对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">      <span class="comment"># 创建光标对象</span></span><br><span class="line">      cs = self.mysql_conn.cursur()</span><br><span class="line">      <span class="comment"># sql_str = 'insert into t_zhaobiao (字段,字段) value ("值","值")'</span></span><br><span class="line">      sql_column = <span class="string">','</span>.join([key <span class="keyword">for</span> key <span class="keyword">in</span> item.keys()])</span><br><span class="line">      sql_value = <span class="string">','</span>.join([<span class="string">'"%s"'</span>% item[key] <span class="keyword">for</span> key <span class="keyword">in</span> item.keys()])</span><br><span class="line">      sql_str = <span class="string">'insert into t_zhaobiao (%s) value (%s);'</span> %(sql_column,sql_value)</span><br><span class="line">      cs.execute(sql_str)</span><br><span class="line"></span><br><span class="line">      self.mysql_conn.commit()</span><br><span class="line">      <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>代码主体修改及补充</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span> <span class="comment"># 可以跳过allow domaine</span></span><br><span class="line">       <span class="keyword">for</span> keyword <span class="keyword">in</span> self.keyword_s:</span><br><span class="line">           form_data = deepcopy(self.form_data)</span><br><span class="line">           form_data[<span class="string">'key'</span>] = keyword</span><br><span class="line">           form_data[<span class="string">'currentPage'</span>] = <span class="string">'1'</span></span><br><span class="line">           request = scrapy.FormRequest(</span><br><span class="line">               url=<span class="string">'http://ss.ebnew.com/tradingSearch/index.htm'</span>,</span><br><span class="line">               formdata=form_data,</span><br><span class="line">               callback=self.parse_start</span><br><span class="line">           )</span><br><span class="line">           request.meta[<span class="string">'form_data'</span>] = form_data <span class="comment"># 把这个对象保存在request对象的meta里</span></span><br><span class="line">           <span class="keyword">yield</span> request</span><br></pre></td></tr></table></figure>


<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><h2 id="bilian-py"><a href="#bilian-py" class="headerlink" title="bilian.py"></a>bilian.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re <span class="comment">#</span></span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy <span class="comment"># 深度复制</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BilianSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'bilian'</span></span><br><span class="line">    allowed_domains = [<span class="string">'ebnew'</span>,<span class="string">'ss.ebnew.com'</span>]</span><br><span class="line"></span><br><span class="line">    keyword_s =[</span><br><span class="line">        <span class="string">'路由器'</span>,<span class="string">'变压器'</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># start_urls = ['http://ebnew/']</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># def parse(self, response):</span></span><br><span class="line">    <span class="comment">#     pass</span></span><br><span class="line">    <span class="comment"># 存储的数据格式</span></span><br><span class="line">    sql_data = dict(</span><br><span class="line">        projectcode=<span class="string">''</span>,  <span class="comment"># 项目编号</span></span><br><span class="line">        web=<span class="string">''</span>,  <span class="comment"># 信息来源网站</span></span><br><span class="line">        keyword=<span class="string">''</span>,  <span class="comment"># 关键字</span></span><br><span class="line">        detail_url=<span class="string">''</span>,  <span class="comment"># 招标详细页网址</span></span><br><span class="line">        title=<span class="string">''</span>,  <span class="comment"># 第三方网站发布标题</span></span><br><span class="line">        toptype=<span class="string">''</span>,  <span class="comment"># 信息类型</span></span><br><span class="line">        province=<span class="string">''</span>,  <span class="comment"># 归属省份</span></span><br><span class="line">        product=<span class="string">''</span>,  <span class="comment"># 产品范畴</span></span><br><span class="line">        industry=<span class="string">''</span>,  <span class="comment"># 归属行业</span></span><br><span class="line">        tendering_manner=<span class="string">''</span>,  <span class="comment"># 招标方式</span></span><br><span class="line">        publicity_date=<span class="string">''</span>,  <span class="comment"># 招标公示日期</span></span><br><span class="line">        expiry_date=<span class="string">''</span>,  <span class="comment"># 招标截止时间</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># Form表单的数据格式</span></span><br><span class="line">    form_data = dict(</span><br><span class="line">        infoClassCodes=<span class="string">''</span>,</span><br><span class="line">        rangeType=<span class="string">''</span>,</span><br><span class="line">        projectType=<span class="string">'bid'</span>,</span><br><span class="line">        fundSourceCodes=<span class="string">''</span>,</span><br><span class="line">        dateType=<span class="string">''</span>,</span><br><span class="line">        startDateCode=<span class="string">''</span>,</span><br><span class="line">        endDateCode=<span class="string">''</span>,</span><br><span class="line">        normIndustry=<span class="string">''</span>,</span><br><span class="line">        normIndustryName=<span class="string">''</span>,</span><br><span class="line">        zone=<span class="string">''</span>,</span><br><span class="line">        zoneName=<span class="string">''</span>,</span><br><span class="line">        zoneText=<span class="string">''</span>,</span><br><span class="line">        key=<span class="string">''</span>,  <span class="comment"># 搜索的关键字</span></span><br><span class="line">        pubDateType=<span class="string">''</span>,</span><br><span class="line">        pubDateBegin=<span class="string">''</span>,</span><br><span class="line">        pubDateEnd=<span class="string">''</span>,</span><br><span class="line">        sortMethod=<span class="string">'timeDesc'</span>,</span><br><span class="line">        orgName=<span class="string">''</span>,</span><br><span class="line">        currentPage=<span class="string">''</span>,  <span class="comment"># 当前页码</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span> <span class="comment"># 可以跳过allow domaine</span></span><br><span class="line">        <span class="keyword">for</span> keyword <span class="keyword">in</span> self.keyword_s:</span><br><span class="line">            form_data = deepcopy(self.form_data)</span><br><span class="line">            form_data[<span class="string">'key'</span>] = keyword</span><br><span class="line">            form_data[<span class="string">'currentPage'</span>] = <span class="string">'1'</span></span><br><span class="line">            request = scrapy.FormRequest(</span><br><span class="line">                url=<span class="string">'http://ss.ebnew.com/tradingSearch/index.htm'</span>,</span><br><span class="line">                formdata=form_data,</span><br><span class="line">                callback=self.parse_start</span><br><span class="line">            )</span><br><span class="line">            request.meta[<span class="string">'form_data'</span>] = form_data <span class="comment"># 把这个对象保存在request对象的meta里</span></span><br><span class="line">            <span class="keyword">yield</span> request</span><br><span class="line"></span><br><span class="line">        <span class="comment"># yield scrapy.Request(</span></span><br><span class="line">        <span class="comment">#     url = 'http://www.ebnew.com/businessShow/645672039.html',</span></span><br><span class="line">        <span class="comment">#     callback = self.parse_page(2)</span></span><br><span class="line">        <span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># form_data = self.form_data</span></span><br><span class="line">        <span class="comment"># form_data['key'] = '路由器'</span></span><br><span class="line">        <span class="comment"># form_data['currentPage'] = '2'</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># yield scrapy.FormRequest( # 对FormRequest的封装，可以提交表单数据</span></span><br><span class="line">        <span class="comment">#     url = 'http://ss.ebnew.com/tradingSearch/index.htm',</span></span><br><span class="line">        <span class="comment">#     formdata= form_data,# 传递字典对象</span></span><br><span class="line">        <span class="comment">#     callback=self.parse_page1,# 定义回调函数</span></span><br><span class="line">        <span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_start</span><span class="params">(self,response)</span>:</span></span><br><span class="line">        a_text_s = response.xpath(<span class="string">'//form[@id="pagerSubmitForm"]/a/text()'</span>).extract()</span><br><span class="line">        page_max = max(</span><br><span class="line">            [int(a_text) <span class="keyword">for</span> a_text <span class="keyword">in</span> a_text_s <span class="keyword">if</span> re.match(<span class="string">'\d+'</span>,a_text)] <span class="comment"># 都是数字的列表取最大值</span></span><br><span class="line">        )</span><br><span class="line">        page_max = <span class="number">2</span></span><br><span class="line">        self.parse_page1(response) <span class="comment"># parse_start 访问的就是第一页，所以要加这个步骤，在进行其他页码的遍历</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">2</span>,page_max+<span class="number">1</span>):</span><br><span class="line">            form_data = deepcopy(response.meta[<span class="string">'form_data'</span>])</span><br><span class="line">            form_data[<span class="string">'currentPage'</span>] = str(page)</span><br><span class="line">            request = scrapy.FormRequest(</span><br><span class="line">                url = <span class="string">'http://ss.ebnew.com/tradingSearch/index.htm'</span>,</span><br><span class="line">                formdata=form_data,</span><br><span class="line">                callback=self.parse_page1</span><br><span class="line">            )</span><br><span class="line">            request.meta[<span class="string">'form_data'</span>] = form_data</span><br><span class="line">            <span class="keyword">yield</span> request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_page1</span><span class="params">(self,response)</span>:</span></span><br><span class="line">        form_data = response.meta[<span class="string">'form_data'</span>]</span><br><span class="line">        keyword = form_data.get(<span class="string">'key'</span>)</span><br><span class="line">        content_list_x_s = response.xpath(<span class="string">'//div[@class="ebnew-content-list"]/div'</span>)</span><br><span class="line">        <span class="keyword">for</span> content_list_x <span class="keyword">in</span> content_list_x_s:</span><br><span class="line">            sql_data = deepcopy(self.sql_data)</span><br><span class="line">            <span class="comment"># 这里需要把下面的数据保存到sql_data里</span></span><br><span class="line">            sql_data[<span class="string">'toptype'</span>] = content_list_x.xpath(<span class="string">'./div[1]/i[1]/text()'</span>).extract_first()</span><br><span class="line">            sql_data[<span class="string">'title'</span>] = content_list_x.xpath(<span class="string">'./div[1]/a[1]/text()'</span>).extract_first()</span><br><span class="line">            sql_data[<span class="string">'publicity_date'</span>] =content_list_x.xpath(<span class="string">'./div[1]/i[2]/text()'</span>).extract_first()</span><br><span class="line">            <span class="keyword">if</span> sql_data[<span class="string">'publicity_date'</span>]:</span><br><span class="line">                sql_data[<span class="string">'publicity_date'</span>] = re.sub(<span class="string">'[^0-9\-]'</span>,<span class="string">''</span>,sql_data[<span class="string">'publicity_date'</span>]) <span class="comment"># 将发布日期里的文字'发布日期'替换成空字符串的格式</span></span><br><span class="line"></span><br><span class="line">            sql_data[<span class="string">'tendering_manner'</span>] = content_list_x.xpath(<span class="string">'./div[2]/div[1]/p[1]/span[2]/text()'</span>).extract_first()</span><br><span class="line">            sql_data[<span class="string">'product'</span>] = content_list_x.xpath(<span class="string">'./div[2]/div[1]/p[2]/span[2]/text()'</span>).extract_first()</span><br><span class="line">            sql_data[<span class="string">'expiry_date'</span>] = content_list_x.xpath(<span class="string">'./div[2]/div[2]/p[1]/span[2]/text()'</span>).extract_first()</span><br><span class="line">            sql_data[<span class="string">'province'</span>] = content_list_x.xpath(<span class="string">'./div[2]/div[2]/p[2]/span[2]/text()'</span>).extract_first()</span><br><span class="line">            sql_data[<span class="string">'detail_url'</span>] = content_list_x.xpath(<span class="string">'./div[1]/a/@href'</span>).extract_first()</span><br><span class="line">            sql_data[<span class="string">'keyword'</span>] = keyword</span><br><span class="line">            sql_data[<span class="string">'web'</span>] = <span class="string">'必联网'</span></span><br><span class="line">            request = scrapy.Request(</span><br><span class="line">                url = sql_data[<span class="string">'detail_url'</span>],</span><br><span class="line">                callback=self.parse_page2</span><br><span class="line">            )</span><br><span class="line">            request.meta[<span class="string">'sql_data'</span>] = sql_data</span><br><span class="line">            <span class="keyword">yield</span> request</span><br><span class="line">            <span class="comment"># print(toptype,title,publicity_date,tendering_manner,product,expiry_date,province)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_page2</span><span class="params">(self,response)</span>:</span></span><br><span class="line">        sql_data = response.meta[<span class="string">'sql_data'</span>]</span><br><span class="line">        sql_data[<span class="string">'projectcode'</span>] = response.xpath(</span><br><span class="line">            <span class="string">'/ul[contains(@class,"ebnew-project-information")]/li[1]/span[2]/text()'</span>).extract_first()</span><br><span class="line">        sql_data[<span class="string">'industry'</span>] = response.xpath(</span><br><span class="line">            <span class="string">'/ul[contains(@class,"ebnew-project-information")]/li[8]/span[2]/text()'</span>).extract_first()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> response.meta[<span class="string">'projectcode'</span>]:</span><br><span class="line">            projectcode_find = re.findall(<span class="string">'项目编号[：:]&#123;0,1&#125;\s&#123;0,2&#125;([a-zA-Z0-9\-]&#123;10,80&#125;)'</span>,response.body.decode(<span class="string">'utf-8'</span>))</span><br><span class="line">            response.meta[<span class="string">'projectcode '</span>]= projectcode_find[<span class="number">0</span>] <span class="keyword">if</span> projectcode_find <span class="keyword">else</span> <span class="string">""</span></span><br><span class="line">        <span class="comment"># print(industry,projectcode)</span></span><br><span class="line">        <span class="keyword">yield</span> sql_data <span class="comment"># 把sql_data转交给管道文件</span></span><br></pre></td></tr></table></figure>
<h2 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhaobiaoPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.mysql_conn = pymysql.connections(</span><br><span class="line">            host = <span class="string">'localhost'</span>,</span><br><span class="line">            post=<span class="number">3306</span>,</span><br><span class="line">            user=<span class="string">'root'</span>,</span><br><span class="line">            password = <span class="string">'126511'</span>,</span><br><span class="line">            database=<span class="string">'zhaobiao'</span>,</span><br><span class="line">            charset=<span class="string">'utf8'</span>,</span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># 创建光标对象</span></span><br><span class="line">        cs = self.mysql_conn.cursur()</span><br><span class="line">        <span class="comment"># sql_str = 'insert into t_zhaobiao (字段,字段) value ("值","值")'</span></span><br><span class="line">        sql_column = <span class="string">','</span>.join([key <span class="keyword">for</span> key <span class="keyword">in</span> item.keys()])</span><br><span class="line">        sql_value = <span class="string">','</span>.join([<span class="string">'"%s"'</span>% item[key] <span class="keyword">for</span> key <span class="keyword">in</span> item.keys()])</span><br><span class="line">        sql_str = <span class="string">'insert into t_zhaobiao (%s) value (%s);'</span> %(sql_column,sql_value)</span><br><span class="line">        cs.execute(sql_str)</span><br><span class="line"></span><br><span class="line">        self.mysql_conn.commit()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>个人项目</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫项目1</title>
    <url>/2020/05/13/%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE1/</url>
    <content><![CDATA[<p>通过在老师带领下操作了爬虫项目爬取csdn的热门博客，希望自己可以试着去爬取bilibili上某一up主的视频题目</p>
<a id="more"></a>

<h1 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h1><h2 id="获取网页内容"><a href="#获取网页内容" class="headerlink" title="获取网页内容"></a>获取网页内容</h2><p>首先，由于是直接找某一up的视频，所以网站解析页可以为他的主页</p>
<blockquote>
<p>https: //space.bilibili.com/</p>
</blockquote>
<p>在主页中，选择选项中的“视频”，可以获得一个新的网址</p>
<blockquote>
<p>https: //space.bilibili.com/99157282/video</p>
</blockquote>
<p>为了寻找到不同页数时所使用的不同网址，因此点击第二、三页得到另一个网址</p>
<blockquote>
<p>https: //space.bilibili.com/99157282/video?tid=0&amp;page=2&amp;keyword=&amp;order=pubdate<br>https: //space.bilibili.com/99157282/video?tid=0&amp;page=3&amp;keyword=&amp;order=pubdate<br>通过对比可以发现在不同页面只有【page=2】更改，因此可以了解到不同页面的区别只是需要更改page的数字即可</p>
</blockquote>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>个人项目</tag>
      </tags>
  </entry>
  <entry>
    <title>Scrapy-Redis分布式爬虫</title>
    <url>/2020/05/14/scrapy-redis/</url>
    <content><![CDATA[<h2 id="Redis-的数据库结构"><a href="#Redis-的数据库结构" class="headerlink" title="Redis 的数据库结构"></a>Redis 的数据库结构</h2><ol>
<li>Redis中有16个数据库db(0) ~ db(15)<br>开启Redis后默认使用db(0)<br>非关系型数据库，所有数据都堆积在一起</li>
<li>切换数据库：<br>select 【index】<br>例如：切换到db1 </li>
</ol>
<blockquote>
<p>select 1<br>3. 终端里<br>$ redis-cli<br>显示端口号：‘27.0.0.1:6379&gt; ’<br>select 1<br>select 0</p>
</blockquote>
<h2 id="Redis的数据类型"><a href="#Redis的数据类型" class="headerlink" title="Redis的数据类型"></a>Redis的数据类型</h2><p>后续上传文件</p>
<h2 id="pycharm-配置"><a href="#pycharm-配置" class="headerlink" title="pycharm 配置"></a>pycharm 配置</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line">redis_conn = redis.StrictRedis(</span><br><span class="line">    host = <span class="string">'localhost'</span>,</span><br><span class="line">    port = <span class="number">6379</span>,</span><br><span class="line">    password=<span class="string">''</span>,</span><br><span class="line">    decode_responses = <span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="运行redis命令"><a href="#运行redis命令" class="headerlink" title="运行redis命令"></a>运行redis命令</h2><figure class="highlight"><table><tr><td class="code"><pre><span class="line">ret = redis_conn.execute_command(<span class="string">'keys *'</span>)</span><br><span class="line">print(ret)</span><br><span class="line">print(type(ret))</span><br><span class="line">redis_conn.execute_command(<span class="string">'set k1 v1'</span>)</span><br><span class="line">print(ret)</span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">[<span class="string">'k3'</span>, <span class="string">'k1'</span>, <span class="string">'k5'</span>, <span class="string">'k2'</span>]</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">list</span>'&gt;</span></span><br><span class="line">['k3', 'k1', 'k5', 'k2']</span><br></pre></td></tr></table></figure>

<h1 id="Scrapy-redis"><a href="#Scrapy-redis" class="headerlink" title="Scrapy-redis"></a>Scrapy-redis</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>pip install scrapy-redis</p>
<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>多个爬虫项目爬取一个网站<br>多台计算机配合爬取一个网站</p>
<p>scrapy-redis 组建实现分布式爬虫<br>不是一种框架，是一个拓展使scrapy可以完成分布式爬虫</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>在scrapy项目中的settings加入下列代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动Scrapy-Redis去重过滤器，取消Scrapy的去重功能</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">"scrapy_redis.dupefilter.RFPDupeFilter"</span></span><br><span class="line"><span class="comment"># 启用Scrapy-Redis的调度器，取消Scrapy的调度器</span></span><br><span class="line">SCHEDULER = <span class="string">"scrapy_redis.scheduler.Scheduler"</span></span><br><span class="line"><span class="comment"># Scrapy-Redis断点续爬</span></span><br><span class="line">SCHEDULER_PERSIST = <span class="literal">True</span></span><br><span class="line"><span class="comment"># 配置Redis数据库的连接</span></span><br><span class="line">REDIS_URL = <span class="string">'redis://127.0.0.1:6379'</span></span><br></pre></td></tr></table></figure>
<p>在这行代码中，需要不断访问代理IP</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">request.meta[<span class="string">'proxy'</span>] = <span class="string">'http://'</span> + ur.urlopen(<span class="string">'http://api.ip.data5u.com/dynamic/get.html?order=d314e5e5e19b0dfd19762f98308114ba&amp;sep=3'</span>).read().decode(<span class="string">'utf-8'</span>).strip() <span class="comment"># 设置代理</span></span><br></pre></td></tr></table></figure>

<h2 id="proxypool-py"><a href="#proxypool-py" class="headerlink" title="proxypool.py"></a>proxypool.py</h2><p>在项目文件夹（非spider）下建立proxypool.py</p>
<ol>
<li>倒入模块<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> urllib.request <span class="keyword">as</span> ur</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure></li>
<li>定义Proxypool类并定义两个方法<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyPool</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.redis_conn = redis.StrictRedis(</span><br><span class="line">            host=<span class="string">'localhost'</span>,</span><br><span class="line">            port = <span class="number">6379</span>,</span><br><span class="line">            decode_responses=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_proxy</span><span class="params">(self)</span>:</span></span><br><span class="line">    	proxy_new = ur.urlopen(<span class="string">'http://api.ip.data5u.com/dynamic/get.html?order=d314e5e5e19b0dfd19762f98308114ba&amp;sep=4'</span>).read().decode(<span class="string">'utf-8'</span>).strip().split(<span class="string">' '</span>)</span><br><span class="line">    	print(proxy)</span><br><span class="line">        <span class="comment"># while True:# 不断循环获取链接</span></span><br><span class="line">        <span class="comment">#     proxy_new = ur.urlopen('http://api.ip.data5u.com/dynamic/get.html?order=d314e5e5e19b0dfd19762f98308114ba&amp;sep=4').read().decode('utf-8').strip().split(' ')# strip：去掉首位空格，split：用空格隔开</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">(self)</span>:</span></span><br><span class="line">     	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    ProxyPool().set_proxy()</span><br></pre></td></tr></table></figure>
<p>此时运行main，会获取到两个被打印的代理IP</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">proxy_odd = <span class="literal">None</span> <span class="comment"># 旧的代理IP地址</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    proxy_new = ur.urlopen(<span class="string">'http://api.ip.data5u.com/dynamic/get.html?order=d314e5e5e19b0dfd19762f98308114ba&amp;sep=4'</span>).read().decode(<span class="string">'utf-8'</span>).strip().split(<span class="string">' '</span>) <span class="comment"># 获取的新的代理IP地址</span></span><br><span class="line">    <span class="keyword">if</span> proxy_new != proxy_odd: <span class="comment"># 如果心就的代理IP不同</span></span><br><span class="line">    	proxy_odd = proxy_new <span class="comment"># 将新的代理IP赋值给旧的代理IP</span></span><br><span class="line">        self.redis_conn.delete(<span class="string">'proxy'</span>) <span class="comment"># 删除链接对象里‘proxy’这个键的值</span></span><br><span class="line">        self.redis_conn.sadd(<span class="string">'proxy'</span>,*proxy_new) <span class="comment"># 向‘proxy’这个键中添加两个代理IP地址（刚刚获取到的），这里相当于执行了两次添加</span></span><br><span class="line">        time.sleep(<span class="number">2</span>) <span class="comment"># 睡眠</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
</search>
